# üìù Rapport de Projet : Pr√©diction du Churn (k-NN)

---
[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/hdmanoach/churn-knn/blob/main/notebooks/churn+.ipynb)

## 1. Introduction

Le but de ce projet est de construire un mod√®le de **classification supervis√©e** pour pr√©dire si un client va quitter (churn) un service t√©l√©com.

Nous utilisons un dataset contenant **7043 clients** avec plusieurs caract√©ristiques : genre, dur√©e d‚Äôabonnement (tenure), type de contrat, facturation, services internet, etc.

L‚Äôobjectif est de pr√©dire la variable cible :

* **Churn = Yes** ‚Üí le client quitte
* **Churn = No** ‚Üí le client reste

---
## üß† M√©thodologie
1. Chargement des donn√©es
2. Nettoyage et traitement
3. Encodage des variables cat√©gorielles (One-Hot)
4. Normalisation (StandardScaler)
5. Split train/test (80/20)
6. Entra√Ænement k-NN
7. Optimisation de k avec GridSearchCV
8. √âvaluation (F1-score, confusion matrix)

## 2. Description des donn√©es

Le dataset contient **21 colonnes** :

* `customerID` : identifiant du client
* `Churn` : variable cible (Yes/No)
* Variables cat√©gorielles :

  * Contract
  * InternetService
  * PaymentMethod
  * OnlineSecurity
  * etc.
* Variables num√©riques :

  * tenure
  * MonthlyCharges
  * TotalCharges

### Distribution de la variable cible

| Classe | Nombre |
| ------ | ------ |
| No     | 5174   |
| Yes    | 1869   |

Le dataset est **d√©s√©quilibr√©** : il y a plus de clients qui restent que de clients qui partent.

---

## 3. Pr√©paration des donn√©es

### 3.1 Nettoyage

* V√©rification des valeurs manquantes
* La colonne `TotalCharges` contenait 11 valeurs manquantes
* Apr√®s traitement, aucune valeur manquante ne reste

---

### 3.2 S√©paration des variables

* Suppression de `customerID` (non utile pour la pr√©diction)
* D√©finition :

```python
X = df.drop(['customerID', 'Churn'], axis=1)
y = df['Churn'].map({'Yes': 1, 'No': 0})
```

* `X` : variables explicatives
* `y` : variable cible (0 = No, 1 = Yes)

---

### 3.3 Encodage des variables cat√©gorielles

Les variables cat√©gorielles ont √©t√© transform√©es en variables num√©riques avec :

```python
pd.get_dummies()
```

Cela permet au mod√®le k-NN de travailler avec des donn√©es num√©riques.

---

### 3.4 Normalisation

Les donn√©es ont √©t√© normalis√©es avec :

```python
StandardScaler()
```

La normalisation est essentielle pour k-NN car ce mod√®le est bas√© sur la **distance** entre les points.

---

## 4. S√©paration Train / Test

Le dataset a √©t√© divis√© en :

* **80% donn√©es d'entra√Ænement**
* **20% donn√©es de test**

Avec :

```python
train_test_split(..., stratify=y)
```

Cela permet de garder la m√™me proportion de classes dans train et test.

---

## 5. Mod√®le k-NN

### 5.1 Mod√®le initial (k = 5)

R√©sultats :

* Accuracy : **0.76**
* F1-score (churn) : **0.54**

Matrice de confusion :

```
[[861, 172],
 [172, 202]]
```

Interpr√©tation :

* 861 clients correctement pr√©dits comme restant
* 202 clients correctement pr√©dits comme churn
* 172 faux positifs
* 172 faux n√©gatifs

---

## 6. Optimisation du mod√®le

Une recherche du meilleur `k` a √©t√© effectu√©e avec :

```python
GridSearchCV
```

Test de k de 1 √† 30 en optimisant le **F1-score**.

### R√©sultats :

* **Meilleur k : 25**
* **Meilleur F1-score : 0.5848**

---

## 7. Mod√®le final (k = 25)

Matrice de confusion :

```
[[884, 149],
 [160, 214]]
```

### Performances

| Classe | Precision | Recall | F1-score | Support |
| ------ | --------- | ------ | -------- | ------- |
| No     | 0.85      | 0.86   | 0.85     | 1033    |
| Yes    | 0.59      | 0.57   | 0.58     | 374     |

* Accuracy : **0.78**
* F1-score (churn) : **0.58**

---

## 8. Analyse des r√©sultats

* Le mod√®le est **tr√®s bon pour pr√©dire les clients qui restent**.
* Il est **moins performant pour d√©tecter les churns**, ce qui est logique car les classes sont d√©s√©quilibr√©es.
* Les faux n√©gatifs (clients qui partent mais non d√©tect√©s) ont diminu√© de **172 √† 160** apr√®s optimisation.

L‚Äôoptimisation du param√®tre `k` a donc am√©lior√© les performances.

---

## 9. Conclusion

Le mod√®le k-NN permet une pr√©diction correcte du churn avec :

* Accuracy : 78%
* F1-score churn : 0.58

L‚Äôoptimisation am√©liore les performances mais des am√©liorations restent possibles.

---

## 10. Am√©liorations possibles

* Tester d‚Äôautres mod√®les :

  * Logistic Regression
  * Random Forest
  * XGBoost
* Utiliser SMOTE pour √©quilibrer les classes
* S√©lection des meilleures variables (feature selection)
* Ajuster le seuil de d√©cision pour am√©liorer le recall
* D√©ployer le mod√®le via une application web (Flask / FastAPI)

---

## 11. Technologies utilis√©es

* Python
* Pandas
* Scikit-learn
* Matplotlib
* Google Colab
## üì• Fichiers

- üìÅ data/ : dataset utilis√©
- üìÅ notebooks/ : analyses et mod√®les pas √† pas

# üìå R√©sum√© rapide

Ce projet montre la mise en place compl√®te d‚Äôun pipeline de machine learning :

* Nettoyage des donn√©es
* Encodage
* Normalisation
* Entra√Ænement
* Optimisation
* √âvaluation

Le mod√®le k-NN optimis√© (k=25) atteint un F1-score de **0.58** sur la classe churn.
